{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c126020",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oem/vcivale/UNI_UCB2/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'h5py'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PatchFromH5Dataset, stratified_split, plot_class_distributions\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrain\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ModelTrainer, TrainingArguments\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodelling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ViT_UCB_Pruning\n",
      "File \u001b[0;32m~/vcivale/UNI_UCB2/src/data/dataset.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mh5py\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'h5py'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import wandb\n",
    "\n",
    "from src.data.dataset import PatchFromH5Dataset, stratified_split, plot_class_distributions\n",
    "from src.rl.train import ModelTrainer, TrainingArguments\n",
    "from src.rl.modelling import ViT_UCB_Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e280d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 50\n",
    "\n",
    "# Questo è il rapporto di pruning usato durante il TRAINING\n",
    "TRAINING_KEEP_RATIO = 0.7 # Aumentato da 0.01 per un training più stabile\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea90e7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PatchFromH5Dataset(\n",
    "    h5_dir='/data/patches/',\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize(IMG_SIZE),\n",
    "        transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d111973a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = dataset.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdf2da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea un DataFrame con indici e label\n",
    "df = pd.DataFrame({\n",
    "    \"index\": np.arange(len(labels)),\n",
    "    \"label\": labels\n",
    "})\n",
    "\n",
    "# Trova il numero di elementi della classe minoritaria\n",
    "min_count = df[\"label\"].value_counts().min()\n",
    "\n",
    "# Per ogni classe, seleziona min_count elementi a caso\n",
    "undersampled_df = (\n",
    "    df.groupby(\"label\", group_keys=False)\n",
    "      .apply(lambda x: x.sample(n=min_count, random_state=42)).reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Mischia gli indici\n",
    "undersampled_indices = undersampled_df[\"index\"].sample(frac=1, random_state=42).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c3ae3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "undersampled_labels = [labels[i] for i in undersampled_indices]\n",
    "\n",
    "trainval_idx, test_idx = train_test_split(\n",
    "    undersampled_indices,\n",
    "    test_size=0.3,\n",
    "    stratify=undersampled_labels,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Ottieni i label corrispondenti per il secondo split\n",
    "trainval_labels = [labels[i] for i in trainval_idx]\n",
    "\n",
    "# Split: train vs val\n",
    "train_idx, val_idx = train_test_split(\n",
    "    trainval_idx,\n",
    "    test_size=0.3,\n",
    "    stratify=trainval_labels,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Crea i subset\n",
    "train_dataset = Subset(dataset, train_idx)\n",
    "val_dataset   = Subset(dataset, val_idx)\n",
    "test_dataset  = Subset(dataset, test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51210115",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_class_distributions(train_dataset, val_dataset, test_dataset, full_dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734e7f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True, num_workers=16, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=False, num_workers=16, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=False, num_workers=16, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6825491",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_num = len(np.unique(dataset.labels))\n",
    "\n",
    "print(f\"Number of classes: {labels_num}\")\n",
    "model = ViT_UCB_Pruning(model_name=\"hf-hub:MahmoodLab/uni\", \n",
    "    pretrained=True, \n",
    "    n_classes=labels_num, \n",
    "    keep_ratio=TRAINING_KEEP_RATIO, # Usato durante il training UCB\n",
    "    exclude_cls=True # Escludiamo sempre il CLS token dal pruning\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca083829",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        run_name=f\"ViT-UCB-Training-keep_ratio-{TRAINING_KEEP_RATIO}\",\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=0.1,\n",
    "        train_batch_size=8,\n",
    "        eval_batch_size=8,\n",
    "        max_steps=-1,\n",
    "        warmup_steps=500,\n",
    "        eval_steps=5000,\n",
    "        save_steps=10000,\n",
    "        logging_steps=300,\n",
    "        fp16=False, # Impostato a False, GradScaler non verrà usato\n",
    "        report_to=\"wandb\", \n",
    "        early_stopping_patience=7, \n",
    "        early_stopping_metric=\"eval/loss\", \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1a9ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "# The scheduler needs max_steps, so we calculate it first\n",
    "num_steps = args.num_train_epochs * (len(train_loader) // args.gradient_accumulation_steps)\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768d1a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = ModelTrainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataloader=train_loader,\n",
    "        eval_dataloader=val_loader,\n",
    "        test_dataloader=test_loader,\n",
    "        class_names=dataset.class_names,           # Pass the class names\n",
    "        optimizers=(optimizer, scheduler),\n",
    "        device= DEVICE\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b158ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avvia il training. Il benchmark di velocità verrà eseguito e loggato automaticamente alla fine.\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new-markdown-cell",
   "metadata": {},
   "source": [
    "## Benchmark di Velocità Post-Training\n",
    "\n",
    "Dopo aver completato l'addestramento, la cella seguente eseguirà un benchmark per misurare la velocità di inferenza del modello potato. Questo test:\n",
    "1. Usa il modello appena addestrato.\n",
    "2. Calcola quali token tenere (`top_k_indices`) basandosi sui punteggi UCB appresi.\n",
    "3. Misura il tempo medio di inferenza usando il metodo `forward_pruned`.\n",
    "4. Logga questo risultato in un nuovo progetto su W&B chiamato `vit-ucb-pruning-final` per un'analisi chiara e separata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "new-benchmark-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Benchmark di Velocità di Inferenza Post-Training ---\n",
    "print(\"***** Inizio Benchmark di Velocità Post-Training *****\")\n",
    "\n",
    "# Parametri per il benchmark\n",
    "INFERENCE_KEEP_RATIO = 0.5  # Rapporto di token da conservare per l'inferenza\n",
    "WARMUP_RUNS = 10\n",
    "BENCHMARK_RUNS = 50\n",
    "\n",
    "trained_model = trainer.model\n",
    "trained_model.eval()\n",
    "\n",
    "# 1. Ottieni gli indici dei token più importanti dal modello addestrato\n",
    "try:\n",
    "    top_k_indices = trained_model.get_top_k_patch_indices(keep_ratio=INFERENCE_KEEP_RATIO)\n",
    "    print(f\"Indici per il pruning calcolati con successo con keep_ratio={INFERENCE_KEEP_RATIO}\")\n",
    "    num_total_tokens = trained_model.pos_embed.shape[1]\n",
    "    num_pruned_tokens = len(top_k_indices)\n",
    "    print(f\"Token conservati per l'inferenza: {num_pruned_tokens}/{num_total_tokens}\")\n",
    "except Exception as e:\n",
    "    print(f\"Errore nel calcolo degli indici di pruning: {e}\")\n",
    "    top_k_indices = None\n",
    "\n",
    "if top_k_indices is not None:\n",
    "    # 2. Crea dati fittizi per il benchmark\n",
    "    batch_size = args.eval_batch_size\n",
    "    dummy_input = torch.randn(batch_size, 3, IMG_SIZE, IMG_SIZE, device=DEVICE)\n",
    "    top_k_indices = top_k_indices.to(DEVICE)\n",
    "\n",
    "    # 3. Esegui il benchmark (con warmup)\n",
    "    print(f\"Esecuzione di {WARMUP_RUNS} warmup runs...\")\n",
    "    with torch.no_grad():\n",
    "        for _ in range(WARMUP_RUNS):\n",
    "            _ = trained_model.forward_pruned(dummy_input, top_k_indices)\n",
    "    if DEVICE.type == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    print(f\"Esecuzione di {BENCHMARK_RUNS} benchmark runs...\")\n",
    "    total_time = 0\n",
    "    with torch.no_grad():\n",
    "        for _ in range(BENCHMARK_RUNS):\n",
    "            start_time = time.perf_counter()\n",
    "            _ = trained_model.forward_pruned(dummy_input, top_k_indices)\n",
    "            if DEVICE.type == 'cuda':\n",
    "                torch.cuda.synchronize()\n",
    "            end_time = time.perf_counter()\n",
    "            total_time += (end_time - start_time)\n",
    "    \n",
    "    avg_time_ms = (total_time / BENCHMARK_RUNS) * 1000\n",
    "    print(f\"\n",
    "--- Risultato Benchmark ---\")\n",
    "    print(f\"Tempo medio di inferenza potata: {avg_time_ms:.3f} ms\")\n",
    "    print(\"---------------------------\")\n",
    "    \n",
    "    # 4. Logga il risultato su un nuovo progetto/run W&B\n",
    "    print(\"Logging della metrica di velocità su Weights & Biases...\")\n",
    "    try:\n",
    "        wandb.init(\n",
    "            project=\"vit-ucb-pruning-final\", \n",
    "            name=f\"{args.run_name}-inference-benchmark\",\n",
    "            config={\n",
    "                \"inference_keep_ratio\": INFERENCE_KEEP_RATIO,\n",
    "                \"benchmark_runs\": BENCHMARK_RUNS,\n",
    "                \"training_run_name\": args.run_name\n",
    "            }\n",
    "        )\n",
    "        wandb.log({\"pruned_inference_avg_time_ms\": avg_time_ms})\n",
    "        wandb.finish()\n",
    "        print(\"Metrica loggata con successo!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Errore durante il logging su W&B: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
