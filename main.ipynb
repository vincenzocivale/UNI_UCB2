{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c126020",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from src.data.dataset import PatchFromH5Dataset, stratified_split, plot_class_distributions\n",
    "from src.rl.train import ModelTrainer, TrainingArguments\n",
    "from src.rl.modelling import ViT_UCB_Pruning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e280d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 50\n",
    "\n",
    "PRUNING_RATIO = 1\n",
    "\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea90e7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PatchFromH5Dataset(\n",
    "    h5_dir='/equilibrium/datasets/TCGA-histological-data/hest/patches/patches/',\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize(IMG_SIZE),\n",
    "        transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d111973a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = dataset.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdf2da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea un DataFrame con indici e label\n",
    "df = pd.DataFrame({\n",
    "    \"index\": np.arange(len(labels)),\n",
    "    \"label\": labels\n",
    "})\n",
    "\n",
    "# Trova il numero di elementi della classe minoritaria\n",
    "min_count = df[\"label\"].value_counts().min()\n",
    "\n",
    "# Per ogni classe, seleziona min_count elementi a caso\n",
    "undersampled_df = (\n",
    "    df.groupby(\"label\", group_keys=False)\n",
    "      .apply(lambda x: x.sample(n=min_count, random_state=42)).reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Mischia gli indici\n",
    "undersampled_indices = undersampled_df[\"index\"].sample(frac=1, random_state=42).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c3ae3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "undersampled_labels = [labels[i] for i in undersampled_indices]\n",
    "\n",
    "trainval_idx, test_idx = train_test_split(\n",
    "    undersampled_indices,\n",
    "    test_size=0.3,\n",
    "    stratify=undersampled_labels,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Ottieni i label corrispondenti per il secondo split\n",
    "trainval_labels = [labels[i] for i in trainval_idx]\n",
    "\n",
    "# Split: train vs val\n",
    "train_idx, val_idx = train_test_split(\n",
    "    trainval_idx,\n",
    "    test_size=0.3,\n",
    "    stratify=trainval_labels,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Crea i subset\n",
    "train_dataset = Subset(dataset, train_idx)\n",
    "val_dataset   = Subset(dataset, val_idx)\n",
    "test_dataset  = Subset(dataset, test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51210115",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_class_distributions(train_dataset, val_dataset, test_dataset, full_dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734e7f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True, num_workers=16, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=False, num_workers=16, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=False, num_workers=16, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6825491",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_num = len(np.unique(dataset.labels))\n",
    "\n",
    "print(f\"Number of classes: {labels_num}\")\n",
    "model = ViT_UCB_Pruning(model_name=\"hf-hub:MahmoodLab/uni\", \n",
    "    pretrained=True, \n",
    "    n_classes=labels_num, \n",
    "    keep_ratio=PRUNING_RATIO,        \n",
    "    exclude_cls=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca083829",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        run_name=f\"ViT-L-UCB-{PRUNING_RATIO}\",\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        learning_rate=0.1,\n",
    "        train_batch_size=8,\n",
    "        eval_batch_size=8,\n",
    "        max_steps=-1,\n",
    "        warmup_steps=500,\n",
    "        eval_steps=5000,\n",
    "        save_steps=10000,\n",
    "        logging_steps=300,\n",
    "        fp16=False,\n",
    "        report_to=\"wandb\", \n",
    "        early_stopping_patience=7, \n",
    "        early_stopping_metric=\"eval/loss\", # Oppure monitora la loss (un valore più basso è meglio)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1a9ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "# The scheduler needs max_steps, so we calculate it first\n",
    "num_steps = args.num_train_epochs * (len(train_loader) // args.gradient_accumulation_steps)\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768d1a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = ModelTrainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataloader=train_loader,\n",
    "        eval_dataloader=val_loader,\n",
    "        test_dataloader=test_loader,\n",
    "        class_names=dataset.class_names,           # Pass the class names\n",
    "        optimizers=(optimizer, scheduler),\n",
    "        device= DEVICE\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b158ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
