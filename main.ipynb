{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c126020",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vcivale/UNI_UCB2/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "\n",
    "from src.utils.hf_utils import download_weights\n",
    "from src.utils.vit_config import inizialize_model\n",
    "from src.data.dataset import PatchFromH5Dataset\n",
    "from src.rl.train import Trainer, TrainingArguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e280d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_STEPS = 10000 \n",
    "LEARNING_RATE = 3e-2\n",
    "WEIGHT_DECAY = 0.0\n",
    "DECAY_TYPE = \"cosine\"\n",
    "WARMUP_STEPS = 500\n",
    "IMG_SIZE = 224 \n",
    "TRAIN_BATCH_SIZE = 8\n",
    "VAL_BATCH_SIZE = 8\n",
    "NUM_CLASSES = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b93ef725",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_WEIGHTS_PATH = \"/equilibrium/datasets/TCGA-histological-data/vit_weights_cache\"\n",
    "weights_path = download_weights(HF_WEIGHTS_PATH)\n",
    "\n",
    "timm_pretrained_state_dict = torch.load(weights_path, map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7b54047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "model = inizialize_model(timm_pretrained_state_dict, num_classes=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea90e7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PatchFromH5Dataset(\n",
    "    h5_dir='/equilibrium/datasets/TCGA-histological-data/hest_dataset/datasets--MahmoodLab--hest/snapshots/cf37675c2006e6dfcdaa084ddeca863d21a8ddbb/patches',\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize(IMG_SIZE),\n",
    "        transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04c3ae3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [dataset.label_to_idx[dataset.sample_to_label[file.replace('.h5','')]]\n",
    "          for (file, _) in dataset.data_index]\n",
    "\n",
    "indices = list(range(len(dataset)))\n",
    "\n",
    "# Split stratificato\n",
    "train_idx, val_idx = train_test_split(\n",
    "    indices,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=labels\n",
    ")\n",
    "\n",
    "# Crea i Subset\n",
    "train_dataset = Subset(dataset, train_idx)\n",
    "val_dataset = Subset(dataset, val_idx)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True, num_workers=16)\n",
    "val_loader = DataLoader(val_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=False, num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6ee05d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1833b989",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    momentum=0.9,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69b82ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DECAY_TYPE == \"cosine\":\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=WARMUP_STEPS,\n",
    "        num_training_steps=NUM_STEPS\n",
    "    )\n",
    "else: # DECAY_TYPE == \"linear\"\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=WARMUP_STEPS,\n",
    "        num_training_steps=NUM_STEPS\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "768d1a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvincenzo-civale\u001b[0m (\u001b[33mvincenzo-civale-universi-degli-studi-di-firenze\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/vcivale/UNI_UCB2/wandb/run-20250714_132343-ohryesi9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vincenzo-civale-universi-degli-studi-di-firenze/vision-transformer-training/runs/ohryesi9' target=\"_blank\">vit_training_notebook_run</a></strong> to <a href='https://wandb.ai/vincenzo-civale-universi-degli-studi-di-firenze/vision-transformer-training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vincenzo-civale-universi-degli-studi-di-firenze/vision-transformer-training' target=\"_blank\">https://wandb.ai/vincenzo-civale-universi-degli-studi-di-firenze/vision-transformer-training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vincenzo-civale-universi-degli-studi-di-firenze/vision-transformer-training/runs/ohryesi9' target=\"_blank\">https://wandb.ai/vincenzo-civale-universi-degli-studi-di-firenze/vision-transformer-training/runs/ohryesi9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vcivale/UNI_UCB2/src/rl/train.py:122: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = GradScaler() if self.args.fp16 else None\n",
      "07/14/2025 13:23:45 - INFO - src.rl.train - Training arguments: TrainingArguments(\n",
      "    name = vit_training_notebook_run,\n",
      "    output_dir = ./output_notebook,\n",
      "    eval_every = 50,\n",
      "    learning_rate = 0.03,\n",
      "    weight_decay = 0.0,\n",
      "    num_steps = 10000,\n",
      "    decay_type = cosine,\n",
      "    warmup_steps = 500,\n",
      "    max_grad_norm = 1.0,\n",
      "    local_rank = -1,\n",
      "    seed = 42,\n",
      "    gradient_accumulation_steps = 1,\n",
      "    fp16 = True,\n",
      "    num_classes = 6,\n",
      "    img_size = 224,\n",
      "    train_batch_size = 8,\n",
      "    n_gpu = 1,\n",
      "    device = cuda:0,\n",
      ")\n",
      "07/14/2025 13:23:45 - INFO - src.rl.train - Total parameters: 303.3M\n"
     ]
    }
   ],
   "source": [
    "training_params = {\n",
    "    \"name\": \"vit_training_notebook_run\",\n",
    "    \"output_dir\": \"./output_notebook\",\n",
    "    \"eval_every\": 50,\n",
    "    \"num_steps\": NUM_STEPS,\n",
    "    \"learning_rate\": LEARNING_RATE, # Passa l'LR se il trainer lo calcola internamente\n",
    "    \"weight_decay\": WEIGHT_DECAY,\n",
    "    \"decay_type\": DECAY_TYPE,\n",
    "    \"warmup_steps\": WARMUP_STEPS,\n",
    "    \"max_grad_norm\": 1.0,\n",
    "    \"local_rank\": -1, # Usa -1 per non distribuito in un singolo notebook\n",
    "    \"seed\": 42,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"fp16\": True, # Abilita o disabilita AMP\n",
    "    \"img_size\": IMG_SIZE, # Necessario per UCB_Count_Score\n",
    "    \"train_batch_size\": TRAIN_BATCH_SIZE, # Necessario per UCB_Count_Score\n",
    "     \"num_classes\": NUM_CLASSES,\n",
    "}\n",
    "\n",
    "args = TrainingArguments(**training_params)\n",
    "\n",
    "trainer = Trainer(\n",
    "    args=args,\n",
    "    model=model,\n",
    "    train_dataloader=train_loader,\n",
    "    eval_dataloader=val_loader,\n",
    "    loss_function=loss_function,\n",
    "    optimizer=optimizer, \n",
    "    scheduler=scheduler \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b158ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/14/2025 13:23:45 - INFO - src.rl.train - ***** Running training *****\n",
      "07/14/2025 13:23:45 - INFO - src.rl.train -   Total optimization steps = 10000\n",
      "07/14/2025 13:23:45 - INFO - src.rl.train -   Instantaneous batch size per GPU = 8\n",
      "07/14/2025 13:23:45 - INFO - src.rl.train -   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "07/14/2025 13:23:45 - INFO - src.rl.train -   Gradient Accumulation steps = 1\n",
      "Epoch 1 Training (0 / 10000 Steps) (loss=X.X):   0%|| 0/2134 [00:00<?, ?it/s]/home/vcivale/UNI_UCB2/src/rl/train.py:310: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast() if self.args.fp16 else torch.no_grad():\n",
      "/home/vcivale/UNI_UCB2/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:182: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "Epoch 1 Training (1 / 10000 Steps) (loss=nan):   0%|| 0/2134 [00:02<?, ?it/s]/home/vcivale/UNI_UCB2/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:332: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n",
      "Epoch 1 Training (50 / 10000 Steps) (loss=nan):   2%|| 49/2134 [00:09<05:32,  6.28it/s]07/14/2025 13:23:55 - INFO - src.rl.train - ***** Running Validation *****\n",
      "07/14/2025 13:23:55 - INFO - src.rl.train -   Num steps = 534\n",
      "07/14/2025 13:23:55 - INFO - src.rl.train -   Batch size = 8\n",
      "/home/vcivale/UNI_UCB2/src/rl/train.py:222: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast() if self.args.fp16 else torch.no_grad():\n",
      "Validating... (loss=nan): 100%|| 534/534 [00:36<00:00, 14.73it/s]\n",
      "07/14/2025 13:24:31 - INFO - src.rl.train - \n",
      "\n",
      "07/14/2025 13:24:31 - INFO - src.rl.train - ***** Validation Results *****\n",
      "07/14/2025 13:24:31 - INFO - src.rl.train -   Global Steps: 50\n",
      "07/14/2025 13:24:31 - INFO - src.rl.train -   Valid Loss: nan\n",
      "07/14/2025 13:24:31 - INFO - src.rl.train -   Valid Accuracy: 0.13147\n",
      "07/14/2025 13:24:33 - INFO - src.rl.train - Saved model checkpoint to [DIR: ./output_notebook]\n",
      "Epoch 1 Training (100 / 10000 Steps) (loss=nan):   5%|| 99/2134 [00:55<05:18,  6.39it/s] 07/14/2025 13:24:41 - INFO - src.rl.train - ***** Running Validation *****\n",
      "07/14/2025 13:24:41 - INFO - src.rl.train -   Num steps = 534\n",
      "07/14/2025 13:24:41 - INFO - src.rl.train -   Batch size = 8\n",
      "Validating... (loss=nan): 100%|| 534/534 [00:35<00:00, 14.85it/s]\n",
      "07/14/2025 13:25:17 - INFO - src.rl.train - \n",
      "\n",
      "07/14/2025 13:25:17 - INFO - src.rl.train - ***** Validation Results *****\n",
      "07/14/2025 13:25:17 - INFO - src.rl.train -   Global Steps: 100\n",
      "07/14/2025 13:25:17 - INFO - src.rl.train -   Valid Loss: nan\n",
      "07/14/2025 13:25:17 - INFO - src.rl.train -   Valid Accuracy: 0.13147\n",
      "Epoch 1 Training (150 / 10000 Steps) (loss=nan):   7%|| 149/2134 [01:39<05:13,  6.33it/s]  07/14/2025 13:25:25 - INFO - src.rl.train - ***** Running Validation *****\n",
      "07/14/2025 13:25:25 - INFO - src.rl.train -   Num steps = 534\n",
      "07/14/2025 13:25:25 - INFO - src.rl.train -   Batch size = 8\n",
      "Validating... (loss=nan): 100%|| 534/534 [00:35<00:00, 15.06it/s]\n",
      "07/14/2025 13:26:00 - INFO - src.rl.train - \n",
      "\n",
      "07/14/2025 13:26:00 - INFO - src.rl.train - ***** Validation Results *****\n",
      "07/14/2025 13:26:00 - INFO - src.rl.train -   Global Steps: 150\n",
      "07/14/2025 13:26:00 - INFO - src.rl.train -   Valid Loss: nan\n",
      "07/14/2025 13:26:00 - INFO - src.rl.train -   Valid Accuracy: 0.13147\n",
      "Epoch 1 Training (200 / 10000 Steps) (loss=nan):   9%|| 199/2134 [02:23<05:13,  6.18it/s]  07/14/2025 13:26:08 - INFO - src.rl.train - ***** Running Validation *****\n",
      "07/14/2025 13:26:08 - INFO - src.rl.train -   Num steps = 534\n",
      "07/14/2025 13:26:08 - INFO - src.rl.train -   Batch size = 8\n",
      "Validating... (loss=nan): 100%|| 534/534 [00:36<00:00, 14.74it/s]\n",
      "07/14/2025 13:26:45 - INFO - src.rl.train - \n",
      "\n",
      "07/14/2025 13:26:45 - INFO - src.rl.train - ***** Validation Results *****\n",
      "07/14/2025 13:26:45 - INFO - src.rl.train -   Global Steps: 200\n",
      "07/14/2025 13:26:45 - INFO - src.rl.train -   Valid Loss: nan\n",
      "07/14/2025 13:26:45 - INFO - src.rl.train -   Valid Accuracy: 0.13147\n",
      "Epoch 1 Training (250 / 10000 Steps) (loss=nan):  12%|| 249/2134 [03:07<05:03,  6.21it/s]  07/14/2025 13:26:53 - INFO - src.rl.train - ***** Running Validation *****\n",
      "07/14/2025 13:26:53 - INFO - src.rl.train -   Num steps = 534\n",
      "07/14/2025 13:26:53 - INFO - src.rl.train -   Batch size = 8\n",
      "Validating... (loss=nan): 100%|| 534/534 [00:35<00:00, 15.01it/s]\n",
      "07/14/2025 13:27:28 - INFO - src.rl.train - \n",
      "\n",
      "07/14/2025 13:27:28 - INFO - src.rl.train - ***** Validation Results *****\n",
      "07/14/2025 13:27:28 - INFO - src.rl.train -   Global Steps: 250\n",
      "07/14/2025 13:27:28 - INFO - src.rl.train -   Valid Loss: nan\n",
      "07/14/2025 13:27:28 - INFO - src.rl.train -   Valid Accuracy: 0.13147\n",
      "Epoch 1 Training (300 / 10000 Steps) (loss=nan):  14%|| 299/2134 [03:51<05:28,  5.58it/s]  07/14/2025 13:27:37 - INFO - src.rl.train - ***** Running Validation *****\n",
      "07/14/2025 13:27:37 - INFO - src.rl.train -   Num steps = 534\n",
      "07/14/2025 13:27:37 - INFO - src.rl.train -   Batch size = 8\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
